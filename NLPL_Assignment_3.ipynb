{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ***Experiment 3 :  Feature Extraction - Introduction to Word Vectorization (One Hot Encoding, Bag of Words(BOW), Count Vectorizer, TF-IDF, Word2Vec,FastText, GloVe)***\n",
        "<hr>\n",
        "\n",
        "**Name:** Aayusha Bhatia (22070122004), Ayan Jain (22070122040)  \n",
        "**Lab:** NLP Lab-1"
      ],
      "metadata": {
        "id": "kZhFoNu6dPZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8Kx6lheMh9fq",
        "outputId": "28648d93-3cf6-4096-ee7a-8eb4b68898ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "text_columns = ['artists', 'album_name', 'track_name', 'track_genre']\n",
        "\n",
        "# text preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "for col in text_columns:\n",
        "    df[col + \"_clean\"] = df[col].apply(preprocess_text)\n",
        "\n",
        "# Combine all cleaned text into one column for feature extraction\n",
        "df['combined_text'] = df[[c + \"_clean\" for c in text_columns]].agg(' '.join, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QSt8x3EsgpPJ",
        "outputId": "5d68e5a1-6cf0-4f75-c840-0a8f31c81d01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE HOT ENCODING\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "one_hot_encoded = ohe.fit_transform(df[['track_genre']])\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded, columns=ohe.get_feature_names_out(['track_genre']))\n",
        "print(\"\\nüìå ONE HOT ENCODING\")\n",
        "print(\"Shape:\", one_hot_df.shape)\n",
        "print(\"Feature Names:\", one_hot_df.columns.tolist()[:10], \"...\")\n",
        "print(one_hot_df.head(5))\n",
        "\n",
        "# BAG OF WORDS (BOW)\n",
        "cv = CountVectorizer()\n",
        "bow_matrix = cv.fit_transform(df['combined_text'])\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=cv.get_feature_names_out())\n",
        "print(\"\\nüìå BAG OF WORDS\")\n",
        "print(\"Shape:\", bow_df.shape)\n",
        "print(\"Feature Names:\", cv.get_feature_names_out()[:10], \"...\")\n",
        "print(bow_df.head(5))\n",
        "\n",
        "# COUNT VECTORIZER\n",
        "\n",
        "count_vec = CountVectorizer(min_df=2)\n",
        "count_matrix = count_vec.fit_transform(df['combined_text'])\n",
        "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vec.get_feature_names_out())\n",
        "print(\"\\nüìå COUNT VECTORIZER (min_df=2)\")\n",
        "print(\"Shape:\", count_df.shape)\n",
        "print(\"Feature Names:\", count_vec.get_feature_names_out()[:10], \"...\")\n",
        "print(count_df.head(5))\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vec.fit_transform(df['combined_text'])\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vec.get_feature_names_out())\n",
        "print(\"\\nüìå TF-IDF\")\n",
        "print(\"Shape:\", tfidf_df.shape)\n",
        "print(\"Feature Names:\", tfidf_vec.get_feature_names_out()[:10], \"...\")\n",
        "print(tfidf_df.head(5))\n",
        "\n",
        "# WORD2VEC\n",
        "tokenized_texts = [t.split() for t in df['combined_text']]\n",
        "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(\"\\nüìå WORD2VEC\")\n",
        "print(\"Vocabulary Size:\", len(w2v_model.wv))\n",
        "print(\"Sample Words:\", list(w2v_model.wv.index_to_key)[:10], \"...\")\n",
        "print(\"Vector for first word:\\n\", w2v_model.wv[list(w2v_model.wv.index_to_key)[0]])\n",
        "\n",
        "# FASTTEXT\n",
        "fasttext_model = FastText(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(\"\\nüìå FASTTEXT\")\n",
        "print(\"Vocabulary Size:\", len(fasttext_model.wv))\n",
        "print(\"Sample Words:\", list(fasttext_model.wv.index_to_key)[:10], \"...\")\n",
        "print(\"Vector for first word:\\n\", fasttext_model.wv[list(fasttext_model.wv.index_to_key)[0]])\n",
        "\n",
        "# GloVe\n",
        "try:\n",
        "    glove_path = \"glove.6B.100d.txt\"\n",
        "    glove_model = {}\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            glove_model[word] = vector\n",
        "    print(\"\\nüìå GloVe\")\n",
        "    print(\"Vocabulary Size:\", len(glove_model))\n",
        "    sample_words = list(glove_model.keys())[:10]\n",
        "    print(\"Sample Words:\", sample_words, \"...\")\n",
        "    print(\"Vector for first word:\\n\", glove_model[sample_words[0]])\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n‚ö†Ô∏è GloVe file not found. Please download glove.6B.100d.txt\")\n",
        "\n",
        "df.to_csv(\"saved_dataset.csv\", index=False)\n",
        "print(\"\\n‚úÖ Preprocessing & Feature Extraction Done. Saved to saved_dataset.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_Pf89EFeiiNs",
        "outputId": "696ab5e9-a47c-414c-c076-d0c815c248c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìå ONE HOT ENCODING\n",
            "Shape: (114000, 114)\n",
            "Feature Names: ['track_genre_acoustic', 'track_genre_afrobeat', 'track_genre_alt-rock', 'track_genre_alternative', 'track_genre_ambient', 'track_genre_anime', 'track_genre_black-metal', 'track_genre_bluegrass', 'track_genre_blues', 'track_genre_brazil'] ...\n",
            "   track_genre_acoustic  track_genre_afrobeat  track_genre_alt-rock  \\\n",
            "0                   1.0                   0.0                   0.0   \n",
            "1                   1.0                   0.0                   0.0   \n",
            "2                   1.0                   0.0                   0.0   \n",
            "3                   1.0                   0.0                   0.0   \n",
            "4                   1.0                   0.0                   0.0   \n",
            "\n",
            "   track_genre_alternative  track_genre_ambient  track_genre_anime  \\\n",
            "0                      0.0                  0.0                0.0   \n",
            "1                      0.0                  0.0                0.0   \n",
            "2                      0.0                  0.0                0.0   \n",
            "3                      0.0                  0.0                0.0   \n",
            "4                      0.0                  0.0                0.0   \n",
            "\n",
            "   track_genre_black-metal  track_genre_bluegrass  track_genre_blues  \\\n",
            "0                      0.0                    0.0                0.0   \n",
            "1                      0.0                    0.0                0.0   \n",
            "2                      0.0                    0.0                0.0   \n",
            "3                      0.0                    0.0                0.0   \n",
            "4                      0.0                    0.0                0.0   \n",
            "\n",
            "   track_genre_brazil  ...  track_genre_spanish  track_genre_study  \\\n",
            "0                 0.0  ...                  0.0                0.0   \n",
            "1                 0.0  ...                  0.0                0.0   \n",
            "2                 0.0  ...                  0.0                0.0   \n",
            "3                 0.0  ...                  0.0                0.0   \n",
            "4                 0.0  ...                  0.0                0.0   \n",
            "\n",
            "   track_genre_swedish  track_genre_synth-pop  track_genre_tango  \\\n",
            "0                  0.0                    0.0                0.0   \n",
            "1                  0.0                    0.0                0.0   \n",
            "2                  0.0                    0.0                0.0   \n",
            "3                  0.0                    0.0                0.0   \n",
            "4                  0.0                    0.0                0.0   \n",
            "\n",
            "   track_genre_techno  track_genre_trance  track_genre_trip-hop  \\\n",
            "0                 0.0                 0.0                   0.0   \n",
            "1                 0.0                 0.0                   0.0   \n",
            "2                 0.0                 0.0                   0.0   \n",
            "3                 0.0                 0.0                   0.0   \n",
            "4                 0.0                 0.0                   0.0   \n",
            "\n",
            "   track_genre_turkish  track_genre_world-music  \n",
            "0                  0.0                      0.0  \n",
            "1                  0.0                      0.0  \n",
            "2                  0.0                      0.0  \n",
            "3                  0.0                      0.0  \n",
            "4                  0.0                      0.0  \n",
            "\n",
            "[5 rows x 114 columns]\n",
            "\n",
            "üìå BAG OF WORDS\n",
            "Shape: (114000, 77591)\n",
            "Feature Names: ['aa' 'aaa' 'aaathelma' 'aabaad' 'aac' 'aachencho' 'aadaddi' 'aadam'\n",
            " 'aadat' 'aadavari'] ...\n",
            "   aa  aaa  aaathelma  aabaad  aac  aachencho  aadaddi  aadam  aadat  \\\n",
            "0   0    0          0       0    0          0        0      0      0   \n",
            "1   0    0          0       0    0          0        0      0      0   \n",
            "2   0    0          0       0    0          0        0      0      0   \n",
            "3   0    0          0       0    0          0        0      0      0   \n",
            "4   0    0          0       0    0          0        0      0      0   \n",
            "\n",
            "   aadavari  ...  zy  zyada  zydeco  zyganka  zykina  zyon  zyra  \\\n",
            "0         0  ...   0      0       0        0       0     0     0   \n",
            "1         0  ...   0      0       0        0       0     0     0   \n",
            "2         0  ...   0      0       0        0       0     0     0   \n",
            "3         0  ...   0      0       0        0       0     0     0   \n",
            "4         0  ...   0      0       0        0       0     0     0   \n",
            "\n",
            "   zyrtckradicalmothz  zysku  zz  \n",
            "0                   0      0   0  \n",
            "1                   0      0   0  \n",
            "2                   0      0   0  \n",
            "3                   0      0   0  \n",
            "4                   0      0   0  \n",
            "\n",
            "[5 rows x 77591 columns]\n",
            "\n",
            "üìå COUNT VECTORIZER (min_df=2)\n",
            "Shape: (114000, 39837)\n",
            "Feature Names: ['aa' 'aaa' 'aabaad' 'aachencho' 'aadaddi' 'aadam' 'aadat' 'aadha'\n",
            " 'aadhavan' 'aadiyilalo'] ...\n",
            "   aa  aaa  aabaad  aachencho  aadaddi  aadam  aadat  aadha  aadhavan  \\\n",
            "0   0    0       0          0        0      0      0      0         0   \n",
            "1   0    0       0          0        0      0      0      0         0   \n",
            "2   0    0       0          0        0      0      0      0         0   \n",
            "3   0    0       0          0        0      0      0      0         0   \n",
            "4   0    0       0          0        0      0      0      0         0   \n",
            "\n",
            "   aadiyilalo  ...  zweig  zweiter  zwerg  zwischen  zyada  zydeco  zykina  \\\n",
            "0           0  ...      0        0      0         0      0       0       0   \n",
            "1           0  ...      0        0      0         0      0       0       0   \n",
            "2           0  ...      0        0      0         0      0       0       0   \n",
            "3           0  ...      0        0      0         0      0       0       0   \n",
            "4           0  ...      0        0      0         0      0       0       0   \n",
            "\n",
            "   zyra  zysku  zz  \n",
            "0     0      0   0  \n",
            "1     0      0   0  \n",
            "2     0      0   0  \n",
            "3     0      0   0  \n",
            "4     0      0   0  \n",
            "\n",
            "[5 rows x 39837 columns]\n",
            "\n",
            "üìå TF-IDF\n",
            "Shape: (114000, 77591)\n",
            "Feature Names: ['aa' 'aaa' 'aaathelma' 'aabaad' 'aac' 'aachencho' 'aadaddi' 'aadam'\n",
            " 'aadat' 'aadavari'] ...\n",
            "    aa  aaa  aaathelma  aabaad  aac  aachencho  aadaddi  aadam  aadat  \\\n",
            "0  0.0  0.0        0.0     0.0  0.0        0.0      0.0    0.0    0.0   \n",
            "1  0.0  0.0        0.0     0.0  0.0        0.0      0.0    0.0    0.0   \n",
            "2  0.0  0.0        0.0     0.0  0.0        0.0      0.0    0.0    0.0   \n",
            "3  0.0  0.0        0.0     0.0  0.0        0.0      0.0    0.0    0.0   \n",
            "4  0.0  0.0        0.0     0.0  0.0        0.0      0.0    0.0    0.0   \n",
            "\n",
            "   aadavari  ...   zy  zyada  zydeco  zyganka  zykina  zyon  zyra  \\\n",
            "0       0.0  ...  0.0    0.0     0.0      0.0     0.0   0.0   0.0   \n",
            "1       0.0  ...  0.0    0.0     0.0      0.0     0.0   0.0   0.0   \n",
            "2       0.0  ...  0.0    0.0     0.0      0.0     0.0   0.0   0.0   \n",
            "3       0.0  ...  0.0    0.0     0.0      0.0     0.0   0.0   0.0   \n",
            "4       0.0  ...  0.0    0.0     0.0      0.0     0.0   0.0   0.0   \n",
            "\n",
            "   zyrtckradicalmothz  zysku   zz  \n",
            "0                 0.0    0.0  0.0  \n",
            "1                 0.0    0.0  0.0  \n",
            "2                 0.0    0.0  0.0  \n",
            "3                 0.0    0.0  0.0  \n",
            "4                 0.0    0.0  0.0  \n",
            "\n",
            "[5 rows x 77591 columns]\n",
            "\n",
            "üìå WORD2VEC\n",
            "Vocabulary Size: 77614\n",
            "Sample Words: ['de', 'vivo', 'ao', 'christma', 'remix', 'love', 'la', 'origin', 'feat', 'vol'] ...\n",
            "Vector for first word:\n",
            " [ 1.9588722   2.4218428  -1.4968333   2.3354642   0.8638569  -1.1011149\n",
            "  0.7959033   3.3544936   0.13170056 -1.1823529   0.71169966 -0.08987929\n",
            "  0.60739213  1.855498    1.8856423  -1.7666318   1.0192658   2.7983634\n",
            "  2.4093235  -1.0150557   0.3801299  -0.5973618   3.008019   -0.10086121\n",
            "  3.0351698  -2.472583   -1.2711158   1.8375746  -0.73978525 -1.1917725\n",
            "  2.9730554  -0.7522126  -0.38886622  0.34403828 -1.0083724   2.5353591\n",
            "  1.3322122   1.3936427  -1.3353181   0.60406727 -3.4369972   2.248723\n",
            "  1.0940236  -1.7699659  -0.78798497  1.1841838  -1.3691156   0.03150248\n",
            " -1.8736722   0.918483    0.3229172  -1.6895216  -2.335091   -0.5339324\n",
            "  0.25112882  0.0933519   1.1104269  -1.1676489  -3.0869298  -0.6374972\n",
            " -0.04578244  0.19753657  0.86853987 -0.9342623  -2.2726429   0.39186415\n",
            "  1.9618838  -0.2524388   0.83130145  3.3453262   0.566539    0.5996274\n",
            "  0.08801617  0.9099674   1.9059353   1.4366232   1.7089701  -1.5037948\n",
            "  0.11665369 -1.1800473   1.1661365  -0.7976464  -0.13225289 -0.26878\n",
            "  0.29527804 -0.9615643   1.2538044  -0.699977    0.6111121  -0.76150167\n",
            "  0.37649506  0.18055145 -0.8050818  -0.7255559   0.11402539  1.1930119\n",
            " -1.1295145  -1.7633629   0.75629336  0.22746015]\n",
            "\n",
            "üìå FASTTEXT\n",
            "Vocabulary Size: 77614\n",
            "Sample Words: ['de', 'vivo', 'ao', 'christma', 'remix', 'love', 'la', 'origin', 'feat', 'vol'] ...\n",
            "Vector for first word:\n",
            " [-7.8577340e-02 -4.9620301e-01 -2.0234399e+00  1.1756088e+00\n",
            "  6.2538385e-03 -2.0220146e-01  7.0269213e+00 -2.5437911e+00\n",
            "  1.3095849e+00 -1.0588765e-01  7.9781371e-01 -3.5627139e+00\n",
            " -4.4370711e-01  5.8327332e+00 -2.1816929e-01  6.7227733e-01\n",
            "  3.1841964e-02  1.0769668e+00 -6.7312729e-01 -1.3934165e-01\n",
            "  4.2666698e-01  2.9841096e+00  1.7284859e+00 -2.6857835e-01\n",
            "  3.4745820e+00 -1.2775187e+00 -3.7784438e+00 -2.3030706e-01\n",
            " -1.2810287e+00  1.3275008e+00  3.1776309e+00  2.5910821e+00\n",
            " -1.1866759e-01 -2.5869763e+00 -1.9248754e+00  7.4927890e-01\n",
            " -1.1792929e+00  5.9853226e-01 -3.6227125e-01 -3.4147136e+00\n",
            " -9.6681076e-01 -1.2624238e+00  1.6250569e+00 -1.9682184e+00\n",
            " -5.3464746e+00  4.8657790e-02 -8.5814482e-01 -1.1621153e+00\n",
            " -5.3281732e+00 -4.9560851e-01 -1.7246744e-01  8.6721014e-03\n",
            " -2.5658078e+00  2.0965219e+00 -1.7003150e+00  1.1931976e+00\n",
            "  5.4643553e-01  2.0604727e+00  2.0597599e+00  2.6302800e+00\n",
            "  1.5735642e+00  3.0212958e+00 -3.0714011e+00 -3.2870748e+00\n",
            "  2.0105040e+00 -6.9021976e-01  1.8788575e-01 -1.1320859e+00\n",
            "  3.2053952e+00  4.3259692e+00  8.0447513e-01  1.8476543e+00\n",
            "  1.9697769e-01 -1.2459507e+00  8.0768907e-01  3.3159754e-01\n",
            "  2.8682935e+00  2.9281020e-01  1.5109750e+00  8.4520757e-01\n",
            " -3.0200634e+00 -3.0385275e+00  7.1431470e+00  3.2388997e+00\n",
            " -1.1274490e+00 -3.1661267e+00 -1.8338617e+00 -3.6934234e-02\n",
            " -1.2555647e+00  2.0542665e+00 -5.5428123e+00 -1.1131338e+00\n",
            "  1.4916141e+00 -1.4455148e+00 -4.1314645e+00  3.6367660e+00\n",
            " -2.8478537e+00 -4.4605130e-01 -2.7633109e+00  1.9245350e-01]\n",
            "\n",
            "üìå GloVe\n",
            "Vocabulary Size: 400000\n",
            "Sample Words: ['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"] ...\n",
            "Vector for first word:\n",
            " [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
            "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
            " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
            " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
            " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
            "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
            "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
            " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
            "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
            "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
            "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
            " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
            " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
            " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
            "  0.8278    0.27062 ]\n",
            "\n",
            "‚úÖ Preprocessing & Feature Extraction Done. Saved to saved_dataset.csv\n"
          ]
        }
      ]
    }
  ]
}